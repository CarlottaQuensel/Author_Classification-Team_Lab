\pdfoutput=1
\documentclass[11pt]{article}
\usepackage{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{graphicx}

\title{Author Prediction for Poetry}

\author{Katrin Schmidt \\
   3197512\\
  \texttt{@ims.uni-stuttgart.de} \\\And
  Carlotta Quensel \\
  3546286 \\
  \texttt{@ims.uni-stuttgart.de} \\}

\begin{document}
\maketitle
\begin{abstract}
Same structure as the whole paper, but in short
\end{abstract}

\section{Introduction}
Short motivation and explanation of relevancy
of your task, research questions/hypothesis
\begin{itemize}
\item what is author classification
\item why poetry
\item research question
\begin{itemize}
\item Goal: find features inherent to poetry\\- is our goal possible\\- which features are good
\item Problem: style features might depend on medium (e.g. Limmerick) more than on the author
\end{itemize}
\item Motivation: ??
\end{itemize}


\section{Method}
\subsection{Description of the Methods}
Generally speaking, a maximum entropy classifier is used for generating a probability distribution based on some training data. Before the classifier begins to train, the probabilities should be equally distributed, since there is no bias towards any label. Therefore, the entropy is maximal in the beginning, in other words, the weights associated with the features are unknown (Nigam et al.: 1999). The formula for the maximum entropy classifier is shown below, where $f_i(y, \pmb{x})$ is a feature and $\lambda_i$ the corresponding weight:

\[p_{\lambda}(y|\pmb{x}) = \frac{\text{exp} \sum_i \lambda_i f_i(y,\pmb{x})}{\sum_{y'} \text{exp} \sum_i \lambda_i f_i (y',\pmb{x})}\]

Furthermore, the maximum entropy classifier presupposes a dependence relation between the features. This means that the classifier is not only able to differentiate between features that are relevant and features that are irrelevant for the classification task, but also to include this information in its classification process (Osborne 2002).
We decided to choose this classifier since we assume that features which match a certain author are relevant whereas features that don't match the author are irrelevant.

For our classification task a feature $f_{i}$ contains a data property paired with a label, where $\vec{x}$ is a document vector and $y$ is a label. More precise the document vector is stored as bag-of-words vector. The feature is 1 if the property occurs together with the label and 0 if not, as you can see below:

\[f_{i}(y,\pmb{x})=\begin{cases}
1& \text{if property of }\pmb{x} \text{ occurs with label y}\\
0& \text{otherwise} \\
\end{cases}\]

The features are learned from data with pointwise mutual information (PMI), which is an association measure that helps to decide whether a feature is informative or not (Bouma 2009). This ensures that only relevant features are considered. By doing so, the classification process works faster and returns more reliable results. The formula for PMI looks like the following:

\[\text{PMI}(x,y)=\text{lg}\frac{P(x,y)}{P(x)P(y)}\]

After learning the features, the classifier assumes some random weights between -10 and +10 for each feature and enters an iterative process to improve the feature weights. The iterative training of the weights is done by calculating the derivation of the weights and adding them to the current weights. Then it checks if the accuracy has been improved. 


\subsection{Corpus Creation}
As a training data we used the collection of the Poetry Foundation which is pulled from kaggle.com as a premade csv-database. The dataset consists of 15 567 poems, written by altogether 3 309 authors. The following graphic shows the distribution of poems per author. \\

\includegraphics[width=0.45\textwidth]{/Users/katrin/Desktop/Master/Team_Lab/Report/Statistics.png}
\\

Since the data includes many authors who wrote between 1 and 5 poems, we used only the 30 most prolific authors to get enough data points per class for the method. Finally we ended up with 1 569 poems which is barely 10 \% from the original dataset.
In order to train our model with the data, we sorted the poems by author, normalized some remaining unicode strings (e.g. "ax0", which represents whitespaces) and tokenized them with the NLTK WordPunctTokenizer. Then we split the data into train and test set and converted the poems into bag-of-word vectors using the vocabulary in the train set.


\section{Experiments}

\subsection{Experimental Design}

Explain how you perform your experiments, which data is used, statistics of data.
\begin{itemize}
\item data statistics (decision for number of authors as hyperparameter)
\item Train/Test split
\item Program:\\
- Hyperparameters: accuracy threshold, track loss \& accuracy, \#{}features/author
\item Baseline (bag of words), Advanced: \#{}verses, \#{}stanzas, rhyme scheme
\end{itemize}

\subsection{Results}
Explain how your model performs, different models or configurations of your models.
\begin{itemize}
\item Feature combinations: baseline=BoW, advanced=all, other=?
\item recall/precision/f$_1$ for all combinations (table)
\end{itemize}

\subsection{Error Analysis}

Given the configurations in the Results section, what are frequent sources of errors
\begin{itemize}
\item specifics and numbers about errors?
\item overprediction of alphabetically first author
\item many authors not predicted (uneven data distribution or bad features)
\item feature weights converge similarly (no real weighting)
\end{itemize}

\section{Summary \& Conclusion}

Explain and summarize your results on a more abstract level. What is good, what is not so
good. What are the main contributions in your experiments?


\section{Future Work}

What did you have in mind what else your
would have liked to experiment with? Other ideas?
\begin{itemize}
\item other models (e.g. Neural Net)
\item other features (Topics from Poetry Foundation website)
\item feature interdependencies/more data analysis
\item genre interaction with author classification (multitask learning?)
\end{itemize}


% custom entries
%\bibliographystyle{acl_natbib}
%\bibliography{custom.bib}


\appendix

\section{Contributions}
Who implemented what?
Who participated in the design of which components?
Who wrote which part of the review?
\section{Declaration of Originality}
\label{sec:appendix}

\end{document}
